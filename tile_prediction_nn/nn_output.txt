lucas-MBP-2:tile_prediction_nn luca$ python nn_train.py 
/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
tiles shape (308172, 16)
quaternions shape (308172, 20)
number of users 57
(61635, 20) (61635, 16)
Iteration 1, loss = 3.70302840
Iteration 2, loss = 3.08928197
Iteration 3, loss = 2.98272204
Iteration 4, loss = 2.92614911
Iteration 5, loss = 2.88919096
Iteration 6, loss = 2.85842698
Iteration 7, loss = 2.83302189
Iteration 8, loss = 2.81032874
Iteration 9, loss = 2.78869094
Iteration 10, loss = 2.77000440
Iteration 11, loss = 2.75073544
Iteration 12, loss = 2.73446817
Iteration 13, loss = 2.71779463
Iteration 14, loss = 2.70445535
Iteration 15, loss = 2.69161898
Iteration 16, loss = 2.67572471
Iteration 17, loss = 2.66484364
Iteration 18, loss = 2.65432762
Iteration 19, loss = 2.64029202
Iteration 20, loss = 2.63100761
Iteration 21, loss = 2.61990369
Iteration 22, loss = 2.61017042
Iteration 23, loss = 2.60115389
Iteration 24, loss = 2.59121098
Iteration 25, loss = 2.58042824
Iteration 26, loss = 2.57144046
Iteration 27, loss = 2.56418008
Iteration 28, loss = 2.55587888
Iteration 29, loss = 2.54559182
Iteration 30, loss = 2.53843296
Iteration 31, loss = 2.53367391
Iteration 32, loss = 2.52419192
Iteration 33, loss = 2.51808374
Iteration 34, loss = 2.51355410
Iteration 35, loss = 2.50495835
Iteration 36, loss = 2.49967096
Iteration 37, loss = 2.49472665
Iteration 38, loss = 2.48945378
Iteration 39, loss = 2.48335599
Iteration 40, loss = 2.48174160
Iteration 41, loss = 2.47300294
Iteration 42, loss = 2.46942911
Iteration 43, loss = 2.46494090
Iteration 44, loss = 2.46086794
Iteration 45, loss = 2.45534118
Iteration 46, loss = 2.44952829
Iteration 47, loss = 2.44682259
Iteration 48, loss = 2.44446658
Iteration 49, loss = 2.44060471
Iteration 50, loss = 2.43383069
Iteration 51, loss = 2.42939137
Iteration 52, loss = 2.42618954
Iteration 53, loss = 2.42487194
Iteration 54, loss = 2.41499053
Iteration 55, loss = 2.41419500
Iteration 56, loss = 2.41076354
Iteration 57, loss = 2.40735719
Iteration 58, loss = 2.40329019
Iteration 59, loss = 2.40467647
Iteration 60, loss = 2.40111709
Iteration 61, loss = 2.39583791
Iteration 62, loss = 2.39379209
Iteration 63, loss = 2.38994700
Iteration 64, loss = 2.38398904
Iteration 65, loss = 2.38205197
Iteration 66, loss = 2.37883806
Iteration 67, loss = 2.37693945
Iteration 68, loss = 2.37567214
Iteration 69, loss = 2.36968284
Iteration 70, loss = 2.36664771
Iteration 71, loss = 2.36771499
Iteration 72, loss = 2.36018825
Iteration 73, loss = 2.35867517
Iteration 74, loss = 2.35442428
Iteration 75, loss = 2.35520775
Iteration 76, loss = 2.35195204
Iteration 77, loss = 2.35126074
Iteration 78, loss = 2.34492157
Iteration 79, loss = 2.34559230
Iteration 80, loss = 2.34019139
Iteration 81, loss = 2.33797518
Iteration 82, loss = 2.33771429
Iteration 83, loss = 2.33529405
Iteration 84, loss = 2.33154625
Iteration 85, loss = 2.32950958
Iteration 86, loss = 2.32909969
Iteration 87, loss = 2.32861805
Iteration 88, loss = 2.32332766
Iteration 89, loss = 2.32264069
Iteration 90, loss = 2.32049389
Iteration 91, loss = 2.31560357
Iteration 92, loss = 2.31303839
Iteration 93, loss = 2.31394933
Iteration 94, loss = 2.31207575
Iteration 95, loss = 2.31035882
Iteration 96, loss = 2.30930507
Iteration 97, loss = 2.30675303
Iteration 98, loss = 2.30365088
Iteration 99, loss = 2.30406812
Iteration 100, loss = 2.29648699
Iteration 101, loss = 2.29999395
Iteration 102, loss = 2.29633118
Iteration 103, loss = 2.29478364
Iteration 104, loss = 2.29341706
Iteration 105, loss = 2.29140693
Iteration 106, loss = 2.29366497
Iteration 107, loss = 2.28859685
Iteration 108, loss = 2.28576259
Iteration 109, loss = 2.28410512
Iteration 110, loss = 2.28271769
Iteration 111, loss = 2.28347277
Iteration 112, loss = 2.27736127
Iteration 113, loss = 2.27829500
Iteration 114, loss = 2.27512668
Iteration 115, loss = 2.27394422
Iteration 116, loss = 2.27319414
Iteration 117, loss = 2.27154100
Iteration 118, loss = 2.26973887
Iteration 119, loss = 2.26879499
Iteration 120, loss = 2.26415295
Iteration 121, loss = 2.26535121
Iteration 122, loss = 2.26359939
Iteration 123, loss = 2.26190141
Iteration 124, loss = 2.26040696
Iteration 125, loss = 2.25831649
Iteration 126, loss = 2.26012447
Iteration 127, loss = 2.25519100
Iteration 128, loss = 2.25502228
Iteration 129, loss = 2.25495758
Iteration 130, loss = 2.25060817
Iteration 131, loss = 2.25371775
Iteration 132, loss = 2.24730953
Iteration 133, loss = 2.24755450
Iteration 134, loss = 2.24843823
Iteration 135, loss = 2.24435474
Iteration 136, loss = 2.24323336
Iteration 137, loss = 2.24180708
Iteration 138, loss = 2.24428682
Iteration 139, loss = 2.23998881
Iteration 140, loss = 2.24270785
Iteration 141, loss = 2.23747378
Iteration 142, loss = 2.23777767
Iteration 143, loss = 2.23453727
Iteration 144, loss = 2.23567028
Iteration 145, loss = 2.23013276
Iteration 146, loss = 2.23151449
Iteration 147, loss = 2.23025543
Iteration 148, loss = 2.22711746
Iteration 149, loss = 2.22799985
Iteration 150, loss = 2.22486375
Iteration 151, loss = 2.22564989
Iteration 152, loss = 2.22612131
Iteration 153, loss = 2.22245635
Iteration 154, loss = 2.22025144
Iteration 155, loss = 2.21890857
Iteration 156, loss = 2.22126625
Iteration 157, loss = 2.21769590
Iteration 158, loss = 2.21535731
Iteration 159, loss = 2.21727017
Iteration 160, loss = 2.21212189
Iteration 161, loss = 2.21585594
Iteration 162, loss = 2.21035126
Iteration 163, loss = 2.21001512
Iteration 164, loss = 2.20940516
Iteration 165, loss = 2.20897934
Iteration 166, loss = 2.20805729
Iteration 167, loss = 2.20684451
Iteration 168, loss = 2.20816819
Iteration 169, loss = 2.20389459
Iteration 170, loss = 2.20354636
Iteration 171, loss = 2.20016468
Iteration 172, loss = 2.19845073
Iteration 173, loss = 2.19907332
Iteration 174, loss = 2.19848608
Iteration 175, loss = 2.19906848
Iteration 176, loss = 2.19604950
Iteration 177, loss = 2.19926995
Iteration 178, loss = 2.19188236
Iteration 179, loss = 2.19636249
Iteration 180, loss = 2.19327885
Iteration 181, loss = 2.19037292
Iteration 182, loss = 2.19094684
Iteration 183, loss = 2.19082286
Iteration 184, loss = 2.19233135
Iteration 185, loss = 2.18666737
Iteration 186, loss = 2.18677452
Iteration 187, loss = 2.18608674
Iteration 188, loss = 2.17885902
Iteration 189, loss = 2.18364819
Iteration 190, loss = 2.18517702
Iteration 191, loss = 2.18183414
Iteration 192, loss = 2.18158039
Iteration 193, loss = 2.18298989
Iteration 194, loss = 2.17826505
Iteration 195, loss = 2.17745747
Iteration 196, loss = 2.17700674
Iteration 197, loss = 2.17526938
Iteration 198, loss = 2.17319579
Iteration 199, loss = 2.17315763
Iteration 200, loss = 2.17231632
Iteration 201, loss = 2.17572123
Iteration 202, loss = 2.17184565
Iteration 203, loss = 2.17286996
Iteration 204, loss = 2.16922789
Iteration 205, loss = 2.17103941
Iteration 206, loss = 2.16827181
Iteration 207, loss = 2.16855376
Iteration 208, loss = 2.16664363
Iteration 209, loss = 2.16730417
Iteration 210, loss = 2.16228026
Iteration 211, loss = 2.16315204
Iteration 212, loss = 2.16597022
Iteration 213, loss = 2.16218995
Iteration 214, loss = 2.16244925
Iteration 215, loss = 2.16054319
Iteration 216, loss = 2.16450220
Iteration 217, loss = 2.15800277
Iteration 218, loss = 2.16053137
Iteration 219, loss = 2.16069027
Iteration 220, loss = 2.15613143
Iteration 221, loss = 2.15740225
Iteration 222, loss = 2.15603706
Iteration 223, loss = 2.15290204
Iteration 224, loss = 2.15425863
Iteration 225, loss = 2.15517389
Iteration 226, loss = 2.15171332
Iteration 227, loss = 2.14928885
Iteration 228, loss = 2.15178006
Iteration 229, loss = 2.14819606
Iteration 230, loss = 2.14840282
Iteration 231, loss = 2.14857124
Iteration 232, loss = 2.14451117
Iteration 233, loss = 2.14580081
Iteration 234, loss = 2.14658254
Iteration 235, loss = 2.14664752
Iteration 236, loss = 2.14438892
Iteration 237, loss = 2.14596326
Iteration 238, loss = 2.14260923
Iteration 239, loss = 2.14250129
Iteration 240, loss = 2.14255487
Iteration 241, loss = 2.14110191
Iteration 242, loss = 2.13930949
Iteration 243, loss = 2.13856217
Iteration 244, loss = 2.13608501
Iteration 245, loss = 2.13889082
Iteration 246, loss = 2.13761571
Iteration 247, loss = 2.13709502
Iteration 248, loss = 2.13157953
Iteration 249, loss = 2.13454403
Iteration 250, loss = 2.13263888
Iteration 251, loss = 2.13108741
Iteration 252, loss = 2.13171020
Iteration 253, loss = 2.13094146
Iteration 254, loss = 2.13168794
Iteration 255, loss = 2.12895563
Iteration 256, loss = 2.12923141
Iteration 257, loss = 2.12631272
Iteration 258, loss = 2.13003818
Iteration 259, loss = 2.12640369
Iteration 260, loss = 2.12657153
Iteration 261, loss = 2.12406214
Iteration 262, loss = 2.12459749
Iteration 263, loss = 2.12452877
Iteration 264, loss = 2.12538966
Iteration 265, loss = 2.12373275
Iteration 266, loss = 2.12093340
Iteration 267, loss = 2.12148796
Iteration 268, loss = 2.12092096
Iteration 269, loss = 2.11860284
Iteration 270, loss = 2.11867463
Iteration 271, loss = 2.12197093
Iteration 272, loss = 2.11848857
Iteration 273, loss = 2.11724507
Iteration 274, loss = 2.11911995
Iteration 275, loss = 2.12018238
Iteration 276, loss = 2.11594969
Iteration 277, loss = 2.11666190
Iteration 278, loss = 2.11470142
Iteration 279, loss = 2.11310446
Iteration 280, loss = 2.11421136
Iteration 281, loss = 2.10950385
Iteration 282, loss = 2.11476464
Iteration 283, loss = 2.11349820
Iteration 284, loss = 2.10544359
Iteration 285, loss = 2.10974940
Iteration 286, loss = 2.10940997
Iteration 287, loss = 2.10613895
Iteration 288, loss = 2.10874071
Iteration 289, loss = 2.10907862
Iteration 290, loss = 2.10364931
Iteration 291, loss = 2.10617805
Iteration 292, loss = 2.10250708
Iteration 293, loss = 2.10359412
Iteration 294, loss = 2.10075348
Iteration 295, loss = 2.10178296
Iteration 296, loss = 2.10583080
Iteration 297, loss = 2.10134195
Iteration 298, loss = 2.09698619
Iteration 299, loss = 2.10150753
Iteration 300, loss = 2.10138787
/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
Best NN training error: 0.219184
Best NN test error: 0.295887
lucas-MBP-2:tile_prediction_nn luca$ 

